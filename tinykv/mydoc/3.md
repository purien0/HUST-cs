# project3
## 3a
在raft层完成leader transfer和conf change的实现。
### Implement leader transfer
此处需要处理新的msg类型，MsgTransferLeader和MsgTimeoutNow，其说明如下：
**MsgTransferLeader**
Local Msg，用于上层请求转移 Leader，Project3 使用。成员如下：

| 成员    | 值                               |
| ------- | -------------------------------- |
| MsgType | pb.MessageType_MsgTransferLeader |
| From    | 由谁转移                         |
| To      | 转移给谁                         |


**MsgTimeoutNow**

Local Msg，节点收到后清空 r.electionElapsed，并即刻发起选举，成员如下：

| 成员    | 值                           |
| ------- | ---------------------------- |
| MsgType | pb.MessageType_MsgTimeoutNow |
| From    | 由谁发的                     |
| To      | 发给谁的                     |

流程：
1.当前leader结点收到MsgTransferLeader
2.leader检查 transferee 合法性，包括id是否合法，日志是否最新。
3.若日志落后，向transferee发送MsgAppend直至日志已更新为最新，注意这段时间内leader不再接受proposal
4.向transferee发送MsgTimeoutNow。
5.transderee接收后立即开始选举。

### Implement conf change

这里使用到类型为EntryConfChange，将其作为普通的entry一样进行复制、commit。当entry提交后，上层就会调用rawNode的addNode、和removeNode方法。这里无需关心调用过程，主要要求实现addNode和removeNode方法。

注意：removeNode执行后，由于结点数量变动，曾经未提交的日志可能存在满足大多数而提交的可能，因此需要更新commit信息。

### debug：

#### leader在Prs不存在
```
=== RUN   TestLeaderTransferToNonExistingNode3A
--- FAIL: TestLeaderTransferToNonExistingNode3A (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
```
解决方法：正如测试样例名，没有考虑到收到该信息的leader在Prs不存在的问题，应提前检查。

#### removeNode中无需在votes中执行remove

```
=== RUN   TestCommitAfterRemoveNode3A
    raft_test.go:1273: expected one committed EntryType_EntryNormal, got []
--- FAIL: TestCommitAfterRemoveNode3A (0.00s)
```

修改：removenode不需要remove votes，只需要在Prs中移除id对应项即可。
或者说，由于votes是一个map[uint]bool,属于原子类型，没有必要用_,ok:=votes[id]检查是否存在某id的映射。

#### follower和candidate要对transferLeader信息进行回应
```
=== RUN   TestLeaderTransferToUpToDateNodeFromFollower3A
    raft_test.go:1458: after transferring, node has state StateLeader lead 1, want state StateFollower lead 2
```
查看代码后发现，测试集中发出的transferLeader msg中from和to是相等的：

```go
    // Transfer leadership to 2.
	nt.send(pb.Message{From: 2, To: 2, MsgType: pb.MessageType_MsgTransferLeader})
```

对于follower和candidate，也要对transferLeader信息进行回应，即将该信息转给自己的leader

```go
	if r.Lead != None {
			m.To = r.Lead
			r.msgs = append(r.msgs, m)
		}
```

## 3b
project3B 要实现 project3A 的上层操作，即 LeaderTransfer 和 Add / Remove 的具体应用，涉及到 ConfChange。同时，3B 还要实现 region 的 split。
### TransferLeader
需更改的代码：peerMsgHandler的proposeRaftCommand。
该raftcmd不需要propose。收到 AdminCmdType_TransferLeader 请求，直接调用rawNode的TransferLeader方法，发送 MsgTransferLeader 到 raft 层即可。虽然无propose，但仍需要对callback进行回应。
### confChange
该raftcmd需要propose，但调用的是d.RaftGroup.ProposeConfChange(cc)，与之前调用的Propose不同，添加条目的data是由pb.ConfChange经过Marshal方法转化而来，Propose则是传入由raft_cmdpb.RaftCmdRequest转化而来的data。和普通指令类似，都需要添加proposal。
可以在context内容中存储了msg的marshal结果。


在handleRaftReady中，entry类型为eraftpb.EntryType_EntryConfChange的就是confChange的条目。

confChange应用过程：
1.检查命令是否重复，即如果节点已在集群中，此时跳过apply。

​2.修改peers。若类型为addnode，则增加peer，否则删除peer。注意删除时需要分辨删除的对象是否为自己， 若为自己，需要用d.destroyPeer() 方法销毁自己，直接返回。
3.​region.RegionEpoch.ConfVer ++
4.更新GlobalContext storeMeta​，包括regions​和regionRanges​。对于addNode，需要更改regionRanges；对于deleteNode，无需更改，因为会在d.destroyPeeer()中执行更改。注意访问和修改时的加锁。
5.持久化修改后的 Region，写入RegionLocalState​（使用​meta.WriteRegionState​）。
6.调用 d.RaftGroup.ApplyConfChange() 方法，修改 raft 内部的 peers 信息，其会调用 3A 中实现的 Add 和 Remove方法。
7. 调用 d.insertPeerCache() 或 d.removePeerCache() 方法，更新缓存；
8. 更新 scheduler 那里的 region 缓存，否则可能遇见 no region 的错误。调用d.HeartbeatScheduler(d.ctx.schedulerTaskSender)即可。


注意在addNode阶段不需要实际创建的一个Peer。先加入到集群当中，Leader发送心跳，转发消息时发现节点不存在，由storeWorker调用maybeCreatePeer()进行实际的创建。注意maybeCreatePeer()要求Leader发送的心跳中commit为0。
### split
split是将过大region在keySpace上以splitKey为界分成两个较小的region，是multi-raft实现的关键。split执行的时机和splitKey的生成由split checker负责，新region的id和peers的id由scheduler分配。

Split命令的Propose过程与ChangePeer也是类似的，不同的是Split命令中包含一个split_key​，代表将当前region按split_key​拆分，因此要检查ErrKeyNotInRegion。

>之前在project2b中对于普通命令没有对ErrKeyNotInRegion检查，此处也需要为除了Snap（Snap命令中不包含key）命令之外的其他普通命令增加检查ErrKeyNotInRegion的代码。

apply的过程则相对复杂，我的实现步骤如下：
1.检查ErrKeyNotInRegion，CheckRegionEpoch，检查Peers和msg的newPeersIds长度是否相等。
2.使用split.NewPeerIds​​创造新的newPeers切片​​，其id为NewPeerIds，StoreId则为原来的StoreId。
3.定义newRegion，split.NewRegionId​​为其regionId​​，将split.SplitKey​为StartKey​​，原先region的endKey为EndKey，peers为之前的newPeers切片。注意不继承regionEpoch信息，ConfVer和Version均为0。
4.修改原先region信息。split.key设置为EndKey。
5.两个region的regionEpoch的Version均自增。
6.更新GlobalContext storeMeta​，包括regions​和regionRanges​。注意两个region都需要作为参数item来执行ReplaceOrInsert。注意加锁。
7.持久化修改后的两个Region，写入RegionLocalState​（使用​meta.WriteRegionState​）。
8.创建并注册新的 peer,流程参考storeworker的maybeCreatepeer。
9.d.SizeDiffHint = 0，d.ApproximateSize = new(uint64)
10.更新 scheduler 那里的 region 缓存，注意两个region都要进行调用。由于d.HeartbeatScheduler(d.ctx.schedulerTaskSender)只能使用d.region，可以仿照其写一个类似的函数d.HeartbeatScheduler_2(newRegion, newPeer)来将newRegion通知 scheduler 。
### 注意事项
#### handleRaftReady中applySnapResult
要根据SaveReadyState返回的applySnapResult来更新peerMsgHandler的region和storeMeta信息。
#### ErrEpochNotMatch和ErrRespRegionNotFound处理

此外，由于add、remove、split等因素的影响，我们应多次对region进行检查，虽然我们已经在onRaftMsg中检查过RegionEpoch，但经过消息封装成日志并同步后，RegionEpoch可能出现问题，因此在handleRaftReady中，当apply每一条entry前都应根据msg.head中的信息进行检查，检查RegionEpoch是否过时，以及是否存在ErrRespRegionNotFound。

### debug

#### cannot step as peer not found
```
2024/08/01 17:08:57.201305 peer_msg_handler.go:287: [error] [region 1] 1 handle raft message error raft: cannot step as peer not found
panic: unmatched peers length
```
修改：1.heartbeat中的commit设为0(maybecreatePeer要求)，否则无法成功createPeer。不要用heartbeat更新commit。
2.SnapShot应用后忘记将pendingSnapShot置为nil，及其它某些SnapShot应用相关问题(未记录，2c时没发现该问题)

#### wireType = 0 for field Header
```
panic: proto: wrong wireType = 0 for field Header
```
解决：unmarshal的对象错误,unmarshal失败，要注意marshal和unmarshal过程一一对应，不要用错。

#### start key > end key
TestConfChangeUnreliable3B
```

panic: start key > end key
```
修复:没有检查splitkey是否在region内，可以使用util.CheckKeyInRegion(splitKey, d.Region())，该方法会考虑到endKey为空的特殊情况。

#### meta corruption detected
```
panic: [region 1] 6 meta corruption detected

goroutine 209 [running]:
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).destroyPeer(0xc1559b1e10)
	/home/purein/桌面/tinykv/kv/raftstore/peer_msg_handler.go:840 +0x29f
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).applyConfChange(0xc1559b1e10, 0xc1559b1b40, 0xc1559b1af8)
	/home/purein/桌面/tinykv/kv/raftstore/peer_msg_handler.go:162 +0x58f
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).process(0xc1559b1e10, {0x1, 0xc, 0xb68, {0xc193106600, 0x24, 0x24}, {}, {0x0, 0x0, ...}, ...})
	/home/purein/桌面/tinykv/kv/raftstore/peer_msg_handler.go:86 +0x78
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleRaftReady(0xc1559b1e10)
	/home/purein/桌面/tinykv/kv/raftstore/peer_msg_handler.go:75 +0x425
github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc0001784e0, 0xc00009eae0, 0x0?)
	/home/purein/桌面/tinykv/kv/raftstore/raft_worker.go:57 +0x439
created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers
	/home/purein/桌面/tinykv/kv/raftstore/raftstore.go:270 +0x17b
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	4.457s
FAIL
GO111MODULE=on go test -v --count=1 --parallel=1 -p=1 ./kv/test_raftstore -run ^TestConfChangeSnapshotUnreliableRecoverConcurrentPartition3B|| true
```
同时在TestConfChangeSnapshotUnreliableRecover3B出现该情况
发现log中出现多次6 starts destroy，6 begin to destroy，可能由于unreliable的特性，在entry中出现好几条相同的remove node的confchange条目。发现在当该peermsghandler停机后，没有停止对entry的处理，修改：在每条entry处理后检查是否停机，如下所示。
``` go
for _, entry := range ready.CommittedEntries {
		d.process(entry)
		if d.stopped {return}
	}
```

#### find no region for xxxx
```
 panic: find no region for 34203030303030303030
goroutine 375 [running]:
github.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).GetRegion(0xc00021e780, {0xc1b80a0650, 0xa, 0x10})
	/home/purein/桌面/tinykv/kv/test_raftstore/cluster.go:279 +0x185
github.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).Request(0x709bfc910ad8?, {0xc1b80a0650, 0xa, 0x10}, {0xc18dbf7688, 0x1, 0x1}, 0x12a05f200)
	/home/purein/桌面/tinykv/kv/test_raftstore/cluster.go:189 +0xde
github.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).Scan(0xb84d39?, {0xc1b80a0650, 0xa, 0x10}, {0xc0000eddb0, 0xa, 0x20})
	/home/purein/桌面/tinykv/kv/test_raftstore/cluster.go:366 +0x21a
github.com/pingcap-incubator/tinykv/kv/test_raftstore.GenericTest.func1(0x4, 0x0?)
	/home/purein/桌面/tinykv/kv/test_raftstore/test_test.go:218 +0x533
github.com/pingcap-incubator/tinykv/kv/test_raftstore.runClient(0x9255ae?, 0xc23343e920?, 0xc23341a5a0?, 0x0?)
	/home/purein/桌面/tinykv/kv/test_raftstore/test_test.go:27 +0x78
created by github.com/pingcap-incubator/tinykv/kv/test_raftstore.SpawnClientsAndWait
	/home/purein/桌面/tinykv/kv/test_raftstore/test_test.go:37 +0x85
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	40.823s
FAIL
```

此处是在cluster.GetRegion阶段报错，可能是处理Snap命令时region := c.GetRegion(key)​获取了错误的region（这种情况有可能是在更新region后，没有及时告知Scheduler）。

修改：
1.在confchage，split或者applySnapShot等造成region改变时都显式调用d.HeartbeatScheduler(d.ctx.schedulerTaskSender)。但仍没有解决该问题。
2.在split阶段，产生了新region和新peer，但是没有告知Scheduler，此处将新region和新peer的信息仿照d.HeartbeatScheduler中步骤传入cluster。
结果：成功通过。

####  test timed out after 10m0s
```
panic: test timed out after 10m0s
```
split checker会依据SizeDiffHint​来判断region承载的数据量是否超出阈值，从而触发split操作。不对其进行修改可能会导致region承载数据量过大，影响效率。
对于Put操作
```
​d.SizeDiffHint += uint64(len(req.Put.Key) + len(req.Put.Value))​
```
对于Delete操作
```
​d.SizeDiffHint -= uint64(len(req.Delete.Key))​
```
在Split中也提到Apply Admin_Split完成后，要对SizeDiffHint​和ApproximateSize​更新.
```
d.SizeDiffHint = 0，d.ApproximateSize = new(uint64)。
```
#### index out of range
index out of range [18446744073709551615] with length 2
概率触发的问题：
```
=== RUN   TestSplitConfChangeSnapshotUnreliableRecover3B
panic: runtime error: index out of range [18446744073709551615] with length 2

goroutine 299 [running]:
github.com/pingcap-incubator/tinykv/raft.(*Raft).sendAppend(0xc1ec80e480, 0x8c)
	/home/purein/桌面/tinykv/raft/raft.go:229 +0x38e
github.com/pingcap-incubator/tinykv/raft.(*Raft).handleAppendResponse(0xc1ec80e480, {0x4, 0x8b, 0x8c, 0x1c, 0x0, 0x8, {0x0, 0x0, 0x0}, ...})
	/home/purein/桌面/tinykv/raft/raft.go:599 +0x19e
github.com/pingcap-incubator/tinykv/raft.(*Raft).LeaderStep(0x8b?, {0x4, 0x8b, 0x8c, 0x1c, 0x0, 0x8, {0x0, 0x0, 0x0}, ...})
	/home/purein/桌面/tinykv/raft/raft.go:579 +0x13b
github.com/pingcap-incubator/tinykv/raft.(*Raft).Step(0x0?, {0x4, 0x8b, 0x8c, 0x1c, 0x0, 0x8, {0x0, 0x0, 0x0}, ...})
	/home/purein/桌面/tinykv/raft/raft.go:455 +0x78
github.com/pingcap-incubator/tinykv/raft.(*RawNode).Step(0xc1ec27fe50, {0x4, 0x8b, 0x8c, 0x1c, 0x0, 0x8, {0x0, 0x0, 0x0}, ...})
	/home/purein/桌面/tinykv/raft/rawnode.go:145 +0xdf
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).onRaftMsg(0xc00013de20, 0xc2097a2880)
	/home/purein/桌面/tinykv/kv/raftstore/peer_msg_handler.go:692 +0x318
github.com/pingcap-incubator/tinykv/kv/raftstore.(*peerMsgHandler).HandleMsg(0xc00013de20, {0xc00013ddf0?, 0x8a?, {0xb6af80?, 0xc2097a2880?}})
	/home/purein/桌面/tinykv/kv/raftstore/peer_msg_handler.go:442 +0x85
github.com/pingcap-incubator/tinykv/kv/raftstore.(*raftWorker).run(0xc1dd4abcc0, 0xc1ec215500, 0xc1559497a8?)
	/home/purein/桌面/tinykv/kv/raftstore/raft_worker.go:54 +0x505
created by github.com/pingcap-incubator/tinykv/kv/raftstore.(*Raftstore).startWorkers
	/home/purein/桌面/tinykv/kv/raftstore/raftstore.go:270 +0x17b
FAIL	github.com/pingcap-incubator/tinykv/kv/test_raftstore	34.238s
```
找到数组越界处代码:
``` go
for i := progress.Next; i <= r.RaftLog.LastIndex(); i++ {
		entries = append(entries, &r.RaftLog.entries[i-firstIndex])
	}
```

此处数组越界，即next<firstIndex,可能是由于entry被truncated，但本人实现使用term()来判断需要发送的entry是否被截断，为何先前在entry中还能找到term呢？本人百思不得其解。
一次修改：修改了raft模块中发送快照的条件，当next<firstIndex时也发送快照。
由出现了重复申请快照的问题 ，即x requesting snapshot重复出现很多次，最终因为打开文件太多等问题程序停止运行。
二次修改：设置为当存在pendingsnapshot时，不接受新的snapShot，也不处理Append​。
不再出现该问题。

#### request timeout--vote
查看日志，可以发现不停地在发送MsgRequestVote，且无法被接收。

通信unreliable，且remove node 到最后两个节点，然后被 remove 的那个正好是 Leader。
entry已经同步到两个结点，Leader更新其commit，并发送append来更新follower的commit，若该信息丢失，且leader应用了该条目调用了destoryPeer来删除自己，follower将永远认为一共有两个节点，且一直选举失败。

修复：在调用destroyPeer之前判断当前情况，若当前一共有两个结点且自己为Leader，则在调用destroyPeer之前多次发送heartBeat来尽量使其commit达到最新。

注意：
1.raft模块内发送的HeartBeat的commit必须为0，这是mayBeCreatePeer的要求。
2.对于处理心跳的函数handleHeartBeat，可以根据commit是否为0来判断是常规的心跳还是为了解决该bug的特殊心跳，并执行不同的程序。
3.对于该特殊心跳的处理，最好不要让它回应(发送heartBeatResponse)，否则连续多次的条件造成的连锁反应可能会增大程序压力。我们发送特殊的心跳仅仅是为了更新commit。
4.也可发送appendEntries等，只要能确保commit正常更新即可。

#### meta corruption detected and request timeout--scan
在HandleRaftReady->SaveReadyState->applySnapShot中，snapShot的底层应用需要向ps.regionSched传入runner.RegionTaskApply，并等待其执行完毕通知notifier，对于regionState的写入应在执行完毕并成功应用之后，不然可能出现没有成功应用却已经更新engine中的regionState的情况。

## 3c
需要完成收集集群regions的心跳和实现region balance调度程序
### processRegionHeartbeat
Scheduler会周期性地根据regionHeartbeat来更新region信息。分为两个阶段，检查region是否最新和更新region。

检查：
若scheduler中存在该region记录，则使用util.IsEpochStale()来检查region是否最新。
若scheduler中不存在该region记录，查看所有与该region重叠的overlapRegions，要求对于每一个overlapRegion，该region记录都要比它新。

更新：
冗余的更新不会影响正确率，每次都根据region进行完全的更新，例如leader count, region count, pending peer count等。
### Schedule
此处实现一个balance-region scheduler，它分析各store中region分布是否存在不平衡的地方，防止某个store上有太多region占据空间，使得store的负载趋于平衡。

具体步骤：
1.找出可能需要操作的store，即suitableStore。要求：isup()正在工作且DownTime() < MaxStoreDownTime。
2.suitableStore按照regionSize降序排列
3.从suitableStore中选出sourceStore。按顺序查看其pending，follwer和leader region，要求sourceStore上能选出一个需移动的region且该store的regionSize尽可能大。
4.从suitableStore中选出targetStore。要求先前选出的region不在该store上且该store的regionSize尽可能小。
5.判断该移动是否有价值。要求sourceStore和targetStore的regionSize之差大于待移动region大小的二倍，即移动后sourceStore的regionSize>=targetStore的regionSize。
6.在targetStore 上创建一个 peer，然后调用 CreateMovePeerOperator 生成转移请求
