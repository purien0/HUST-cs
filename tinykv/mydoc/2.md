# project2
## project2a
该部分需要完成raft库的实现，分为2aa领导选举，2ab日志复制，2ac rawNode三个部分。
整体上，从上层到下层依次为rawNode，raft模块和raftlog。

- **rawNode**：该模块用来接收上层传来的信息，将信息下传给 Raft 模块。同时接受 Raft 的同步结果然后传递给上层。RawNode 要负责检查 Raft 模块是否已有同步好的日志需要应用、是否有 Msg 需要发送、是否有日志需要持久化等等，然后把这些信息（Ready）交给上层，上层会据此进行处理。
- **raft**：这是raft算法的具体实现层，领导人选举、日志复制、角色转换等等均是在这里实现
- **raftlog**：用来存储节点的日志信息，不仅存储条目本身，同时还要维护一些信息，如 committed、applied 等等。

### project 2aa 2ab
此处需要实现领导选举和日志复制。
#### raft模块
Raft类定义各成员解析如下：

- **id**：节点id
- **Term**：任期
- **Vote**：当前任期投票给了谁
- **RaftLog**：缓存日志条目
- **Prs**：用于leader维护各节点日志复制的进度（Match代表已经匹配的Index，Next代表即将发送的下一个Index）
- **State**：节点状态
- **votes**：记录投票信息
- **msgs**：需要发送的消息，消息分本地消息（本节点处理）和普通消息
- **Lead**：只想当前的leader
- **heartbeatTimeout**：心跳超时。当heartbeatElapsed>=heartbeatTimeout时即超时。
- **electionTimeout**：选举超时，超时判断同上。
- **heartbeatElapsed**：记录距上次心跳超时经过的时间，用于判断是否心跳超时
- **electionElapsed**：记录，用于判断是否选举超时
- **leadTransferee**、**PendingConfIndex**：在3A领导权转移和配置变更中使用，此处忽略
##### newRaft
使用conf中的内容来初始化raft，注意使用storage的newlog来raftlog，用Storage.InitialState()得到hardState和confState，按照释义初始化其他成员，注意msg需要初始化为nil而非空切片。
##### becomexxx
三个身份转换函数，分别对应Follower，Candidate，Leader。

- becomeFollower​：需要更新Vote​、State​、Term​、Lead​，清空votes​、leadTransferee​，重置两个超时时间，最后随机化选举超时。

- becomeCandidate​：根据Raft算法，要自增Term​，然后先投票给自己（如果当前集群只有一个节点，直接becomeLeader​）。也要重置超时，随机化选举超时。按理说，一旦成为 candidate，会立刻向所有节点发送请求，但是在step()处理msghup发起选举的时候candidate才会发送投票请求。

- becomeLeader：除了状态的设置和各结构体成员的重置，还需要初始化r.Prs中各peer的next和match，becomeLeader的同时发送空日志，最后尝试更新commit。

##### tick()
这是一个逻辑时钟，会对electionElapsed和heartbeatElapsed（leader）进行自增，并检测两者是否达到超时条件。
若选举超时，则将electionElapsed置零后尝试发起选举。
若Leader心跳超时，则将heartbeatElapsed置零后发送心跳。
##### Step()
Step​结合当前节点身份状态r.State​和消息类型MsgType​选择要执行的动作。这部分是实现Raft层的重点，首先要理解各个Msg的作用以及含义。

文档中提到，对于一个Raft节点来说，Msg分本地消息和普通消息，其中普通消息是要发给其他节点的，要放到r.msgs​中供上层异步地取用；本地消息是本地发起的，因此套个Step()​直接进行消息处理。之前在tick()​中发起的就是本地消息MsgHup​和MsgBeat。

首先根据当前结点身份状态，分为XXXStep。之后再根据MsgType执行对应的处理程序，其处理程序是重点，将在接下来的msg处理进行阐述。
#### Raftlog
Raftlog结构体各成员释义如下所示：
- **storage**：自从上次快照以来，所有的持久化条目；
- **commited**：论文中的 committedIndex，即节点认为已经提交的日志 Index；
- **applied**：论文中的 lastApplied，即节点最新应用的日志 Index；
- **stabled**：节点已经持久化的最后一条日志的 Index；
- **entries**：所有未被 compact 的日志，包括持久化与非持久化；
- **pendingSnapshot**：待处理快照，在project2c中使用；
- **lastAppend**：该字段是我自己加的，用于节点记录上一条追加的日志 Index，因为在 follower 更新自己的 committed 时，需要把 leader 传来的 committed 和其进行比较；

关于applied、committed、stabled之间关系，以及在entries中位置示意如下所示：

 snapshot/first.....applied....committed....stabled.....last
  --------------|-------------------log entries----------------------|

  其中log entries从上次snapshot被截断的部分（truncated Index）之后开始。

需要实现的方法如下：

- newLog
- unstableEntries
- nextEnts
- LastIndex
- FirstIndex
- Term

其中newLog根据参数storage来初始化raftlog，其中firstIndex,lastIndex,entries,hardState都通过调用storage方法来获取，之后填写结构体内容即可。
此外其他方法，只要按照注释的要求实现，分清楚log中applied、committed、stabled等概念去实现就好，这里不再赘述。

#### msg处理
Msg 分为 Local Msg 和 Common Msg。前者是本地发起的 Msg，多为上层传递下来的，比如提交条目、请求心跳、请求选举等等，这些 Msg 不会在节点之间传播，它们的 term 也相应的等于 0。后者就是节点之间发送的 Msg，用来集群之间的同步。

TinyKV 通过 pb.Message 结构定义了所有的 Msg，根据MsgType 来区分不同 Msg ，不同Msg的同一成员的释义可能不同。

下面将根据msgType依次分析如何处理。

##### MsgHup

Local Msg，用于请求节点开始选举，可以加入from和to，但是没必要。

| 成员    | 作用                  |
| ------- | --------------------- |
| MsgType | 不用于节点间通信，仅用于发送给本节点让本节点进行选举 |

当节点收到该 Msg 后，会进行相应的判断，如果条件成立，即刻开始选举。判断流程大致为：

1. 判断 r.Prs[r.id] 是否存在。由于网络原因，可能节点收到 MsgHup 之前，自己已经被移除了，此时直接返回即可；
2.判断是否有 pendingSnapshot。如果正在 applying snapshot，则不要发起选举，因为一定是其他的 leader 发给你 snapshot，如果你发起选举，你的 term+1 必定大于现有的 leader，这会造成集群的不稳定。
3. 判断 region 中是否只有自己一个节点。如果是，不用选举，直接成为 Leader。
4. becomeCandidate；
5. 向其他所有节点发送 MsgRequestVote；

##### MsgBeat

Local Msg，用于告知 Leader 该发送心跳了，仅仅需要一个成员。

| 成员    | 作用                     |
| ------- | ---------------------- |
| MsgType | pb.MessageType_MsgBeat |

当 Leader 接收到 MsgBeat 时，向其他所有节点发送心跳。而非 Leader 接收到 MsgBeat 时，直接忽略。

##### MsgPropose

Local Msg，用于上层请求 propose 条目。成员如下：

| 成员    | 作用                      |
| ------- | ------------------------- |
| MsgType | pb.MessageType_MsgPropose |
| Entries | 要 propose 的条目         |
| To      | 发送给哪个节点            |

该 Msg 只有 Leader 能够处理，其余角色收到后直接返回 ErrProposalDropped。Leader 的处理流程如下：

1. 判断 r.leadTransferee 是否等于 None，如果不是，返回 ErrProposalDropped，因为此时集群正在转移 Leader（3A）；
2. 把 m.Entries append到自己的 Entries 中；
3. 向peers发送追加日志 RPC，即 MsgAppend，用于集群同步；
4. 如果集群中只有自己一个节点，则直接更新自己的 committedIndex；

##### MsgAppend

Common Msg，用于 Leader 给其他节点同步日志条目，成员如下：

| 成员    | 作用                                                      |
| ------- | --------------------------------------------------------- |
| MsgType | pb.MessageType_MsgAppend                                  |
| To      | 目标节点                                                  |
| From    | 当前节点id（LeaderId）                                      |
| Term    | 当前节点的 Term                                           |
| LogTerm | 要发送的条目的前一个条目的 Term，即论文中的 prevLogTerm   |
| Index   | 要发送的条目的前一个条目的 Index，即论文中的 prevLogIndex |
| Entries | 要发送的日志条目                                          |
| Commit  | 当前节点的 committedIndex                                 |


处理流程如下：

1.任期检查。Msg的Term若大于自己的Term，说明自己任期落后，更新任期；若小于，说明该消息落后，拒绝。
2.状态检查。若state为candidate，则becomefollower；若为leader，拒绝（这种情况不应该发生）。
3.领导人检查。若自己的lead与Msg发送者不同，则将其改为Msg.from。
4.日志匹配检查。若prevLogIndex > r.RaftLog.LastIndex()或接收者日志中没有包含这样一个条目：即该条目的任期在 prevLogIndex 上能和 prevLogTerm 匹配上，则拒绝。
5.追加新条目，同时删除冲突条目，冲突条目的判别方式和论文中的一致。
6.更新commit，并接受，返回MsgAppendResponse。

前三点的检查会在代码中多次出现，之后不再详述。

注意：不论是接受还是拒绝，都要返回一个 MsgAppendResponse 给 Leader，让 Leader 知道追加是否成功并且更新相关信息。

##### MsgAppendResponse

Common Msg，用于节点告诉 Leader 日志同步是否成功，和 MsgAppend 对应，结构体成员如下：

| 成员    | 值                                                         |
| ------- | ---------------------------------------------------------- |
| MsgType | pb.MessageType_MsgAppendResponse                           |
| Term    | 当前节点的 Term                                            |
| To      | to                                                         |
| Reject  | 是否拒绝                                                   |
| From    | 当前节点的 Id                                              |
| Index   | r.RaftLog.LastIndex()；该成员用于 Leader 更快地去更新 next |

发送：

1不管节点接受或是拒绝，都要发送一个 MsgAppendResponse 给 Leader，调整 Reject 成员即可，其他成员固定；

接收与处理：

1. 只有 Leader 会处理该 Msg，其余角色直接忽略； 
2. 如果被 reject 了，那么重置 next。重置规则为将旧的 next --，然后比较 m.Index + 1，最后取小的那个赋值给 next，然后重新进行日志 / 快照追加；
3. 如果没有 reject，则更新 match 和 next。next 赋值为 m.Index + 1，match 赋值为  next - 1 ；
4. 按照论文的思路更新 commit。即：假设存在 N 满足N > commitIndex，使得大多数的 matchIndex[i] ≥ N以及log[N].term == currentTerm 成立，则令 commitIndex = N。

##### MsgRequestVote

Common Msg，用于 Candidate 请求投票，成员如下：

| 成员    | 值                            |
| ------- | ----------------------------- |
| MsgType | pb.MessageType_MsgRequestVote |
| Term    | 当前节点的 Term               |
| LogTerm | 节点的最后一条日志的 Term     |
| Index   | 节点的最后一条日志的 Index    |
| To      | 发给谁                        |
| From    | 当前节点的 Id                 |

发送：

1. 当节点开始选举并成为 Candidate 时，立刻向其他所有节点发送 MsgRequestVote；

接收与处理：

1. 任期检查。判断 Msg 的 Term 是否大于等于自己的 Term，若是则更新term和voteFor，否则拒绝；注意节点在每个任期有且仅有一票。
2. 如果 votedFor 不为空或者不等于 candidateID，则说明该节点以及投过票了，直接拒绝。否则往下；
3. Candidate 的日志至少和自己一样新，那么就给其投票，否则拒绝。新旧判断逻辑如下：
   - 如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新
   - 如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新；

Candidate 会通过 r.votes 记录下都有哪些节点同意哪些节点拒绝，每次收到MsgRequestVoteResponse都会统计票数，当同意的票数过半时，即可成为 Leader，当拒绝的票数过半时，则转变为 Follower。

##### MsgRequestVoteResponse

Common Msg，用于节点告诉 Candidate 投票结果，成员如下：

| 成员    | 值                                     |
| ------- | -------------------------------------- |
| MsgType | pb.MessageType_MsgRequestVoteResponsev |
| Term    | 当前节点的 Term                        |
| Reject  | 是否拒绝                               |
| To      | 发给谁                                 |
| From    | 当前节点 Id                            |

发送：

1. 节点收到 MsgRequestVote 时，会将结果通过 MsgRequestVoteResponse 发送给 Candidate；

接收与处理：

1. 只有 Candidate 会处理该 Msg，其余节点收到后直接忽略；
2. 根据 m.Reject 更新 r.votes[m.From]，即记录投票结果；
3. 算出同意的票数 agrNum 和拒绝的票数 denNum；
4. 如果同意的票数过半，那么直接成为 Leader；
5. 如果拒绝的票数过半，那么直接成为 Follower；


##### MsgHeartbeat

Common Msg，即 Leader 发送的心跳。不同于论文中使用空的追加日志 RPC 代表心跳，TinyKV 给心跳一个单独的 MsgType。其成员如下：

| 成员    | 值                          |
| ------- | --------------------------- |
| MsgType | pb.MessageType_MsgHeartbeat |
| Term    | 当前节点的 Term             |
| Commit  | util.RaftInvalidIndex       |
| To      | 发给谁                      |
| From    | 当前节点 ID                 |

其中，Commit 成员必须固定为0，与project3B中confchange内容有关。不要用心跳的Commit来更新peer的commit。

发送：

1. 每当 Leader 的 heartbeatTimeout 达到时，就会给其余所有节点发送 MsgHeartbeat；

接收与处理：

1. 任期检查。
2. 重置选举计时  r.electionElapsed
3. 发送 MsgHeartbeatResponse

##### MsgHeartbeatResponse

Common Msg，即节点对心跳的回应。成员如下：

| 成员    | 值                                  |
| ------- | ----------------------------------- |
| MsgType | pb.MessageType_MsgHeartbeatResponse |
| Term    | 当前节点的 Term                     |
| To      | 发给谁                              |
| From    | 当前节点 ID                         |
| Commit  | 当前节点的 committedIndex           |

其中，Commit 成员用于告诉 Leader 自己是否落后。

发送：

1. 当节点收到 MsgHeartbeat 时，会相应的回复 MsgHeartbeatResponse；

接收与处理：

1. 只有 Leader 会处理 MsgHeartbeatResponse，其余角色直接忽略；
2. 通过 m.Commit 判断节点是否落后了，如果是，则进行日志追加；

**MsgSnapshot**在2c中使用，**MsgTransferLeader**、**MsgTimeoutNow**在project3中使用，在此不再叙述。



#### 注意事项
##### 测试要求 msg 是nil，而不是空切片

TestRawNodeRestartFromSnapshot2C 中，want 里的 msg 为 nil，即测试点预期 newRaft 处的 msg 应该为 nil，而不是 make 一个空切片。

##### becomeCandidate不会发送投票

按理说，一旦成为 candidate，会立刻向所有节点发送请求，但是在step()处理msghup发起选举的时候candidate才会发送请求。

##### becomeLeader的同时发送空日志
可以用于更新prs中follower的记录，同时更新follower commit。这与安全性相关，即新leader不负责之前任期的提交更新，但可以用新的一条日志保护起来，使得前一条提交。

raft在当前任期内能够commit之前任期中已经复制给大多数节点但没有commit的日志条目吗?

Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导者当前任期里的日志条目通过计算副本数目可以被提交；**一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。**
![alt text](./assets/image.png)

##### 只有一个结点时直接成为leader
leader在接受到RequestVoteResponse之后统计票数，但是当只有一个结点时没有发出requestVote，自然没有response。需要特别考虑该情况。
##### leader更新commit后发送append来更新follower的commit

##### handleAppendEntry中条目的删除
删除冲突条目后,要注意冲突条目可能存储到storage中，注意修改stable值
##### 切片边界检查
log中函数的实现，如term，切片左闭右开；注意下溢（ok）
storage.Entries(firstIndex, lastIndex+1)后面需要加1

#### 感受：
1.raft很复杂，理清思路，想好再写。
2.注意模块化编程清楚调用关系，理解函数用途。

### 2ac rawnode
RawNode是raft库的接口，把raft库作为一个黑箱来看，其输入是Propose，传递一个消息，其输出是Ready，将会返回一个Ready结构体，从而可以持久化日志和进行apply。

``` go
// RawNode is a wrapper of Raft.
type RawNode struct {
	Raft *Raft
	// Your Data Here (2A).
	PreSoftState *SoftState
	PreHardState pb.HardState
}
```

加入了两个新成员，PreSoftState和PreHardState，用来判断raft是否有更新。

当客户端需要向raft中发送消息时，他会调用Propose，其调用Step处理Msg。客户端将会不断地调用HasReady，判断出是否有需要处理的数据被更新，如果有那么调用Ready将这些数据通过Ready结构传递出来。在对Ready进行处理之后，会调用Advance告知raft某个Ready已经被处理，在raft中可以更新对应结构了。

Ready​结构体成员如下：

- SoftState​：包含Lead​和RaftState；
- pb.HardState​：包含Term​、Vote​、Commit；​
- Entrise​：存放还没持久化的所有条目
- Snapshot​：存放需要应用的snapshot
- CommittedEntries​：存放已经提交但是还没有被应用的日志条目
- Messages​：存放raft节点中需要转发的一些消息（RequestVote、AppendEntries等等）


需实现的函数：
#### NewRawNode
使用参数config调用newraft()来初始化raft，将初始的softstate和hardstate作为PreSoftState、PreHardState。

#### HasReady
检查是否存在ready供上层更新。如果 HasReady() 返回 true，那么上层就会调用 Ready()。
检查时就按照结构体成员的含义，逐一检查是否由需要更新的地方，只要有任何一个成员需要更新便返回true。

需更新的有：SoftState，HardState，unstable entries的存储，已committed未apply的日志的apply，msg的处理。

其中SoftState和HardState​需要和PreSoftState、PreHardState比较来判断是否更新，并记得及时更新PreSoftState、PreHardState。

#### Ready
按照释义为ready各成员赋值，返回一个ready结构体。
注意Ready中SoftState在（SoftState）无更新时值为nil，HardState在(HardState)无更新时为nil。而不是整体上无更新(!HasReady())。这是一处文档中稍有歧义的地方。
#### Advance
当上层处理完 Ready 后，调用 Advance() 来推进整个状态机。

执行advance时，认为上层函数已经完成了ready提供的所有需要更新的部分，而RawNode中的raft还没有进行对应的更新。更新hardstate，stable，apply，以及msg置空即可，softstate在处理上层msg时已经更新了。

#### debug：
主要是理解有误，注意Ready中SoftState在（SoftState）无更新时值为nil，HardState在(HardState)无更新时为nil，两者无联系，不会相互影响，更不是整体上无更新(!HasReady())。
## 2b
project2b需要实现对rawNode之上的应用，peer
首先需要了解Store​, Peer​ 和 Region三个术语。
- Store 代表 tinykv-server 的实例
- Peer 代表运行在 Store 上的 Raft 节点
- Region 是 Peers 的集合，也称为 Raft Group（Region将键空间以范围range划分，是实现Project3 MuitiRaft的关键概念，Project2中默认只有一个Region）
![alt text](./assets/image-1.png)

消息自上而下的传输过程如下：
1.client发送请求RaftCmdRequest，并由peer接收。
2.peer将请求以entry的形式传递给下层raft层。（proposeRaftCommand）
3.raft层内部进行同步。（project2a内容）

同时，peer会不时调用hasReady查看raft层是否有需要apply、持久化的内容(HasReady()?)
### Implement peer storage
搞清楚如何持久化，在/kv/raftstore/meta文件夹下有keys.go和values.go，里面分别是存/取所用的方法，在持久化时调用WriteBatch.SetMeta(key,msg)时需传入的key需要使用keys.go的对应函数转化成正确的格式。
PeerStorage 负责该 peer 的存储。TinyKV 这里提供了两个 badger 实例，分别是 raftDB 和 kvDB，每个实例负责存储不同的内容：

raftDB 存储：

- Ready 中需要 stable 的 entries。
- RaftLocalState

kvDB 存储：

- key-value
- RaftApplyState
- RegionLocalState

需要完成的函数：
#### Append()
在SaveReadyState()中调用的函数，用来存储新增的raftlog以及删去不会被提交的raftlog，实现Entries的持久化。
可以分别得到现有的log首尾Index以及ready.entries的首尾Index来判断，依次注意以下情况：

1.需要存储的raftlog已经被截断truncated，无需存储
2.需要存储的raftlog部分被截断
3.删除后面不会被提交的raftlog

#### SaveReadyState()
该函数用来持久化 Ready 中的数据。将raft.Ready​中更新的数据保存到 badger 中（即持久化），包括追加日志条目、保存 Raft HardState、应用Snapshot（2C）等。

1.通过 raft.isEmptySnap() 方法判断是否存在 Snapshot，如果有，则调用 ApplySnapshot() 方法应用（2C）；
2.保存 ready 中的 HardState 到 ps.raftState.HardState，注意先使用 raft.isEmptyHardState() 进行判空；
3.调用 Append() 将需要持久化的 entries 保存到 raftDB；
4.持久化 RaftLocalState 到 raftDB；
5.通过 raftWB.WriteToDB 和 kvWB.WriteToDB 进行底层写入。

### Implement Raft ready process

#### proposeRaftCommand
将client发送的请求（类型为 *raft_cmdpb.RaftCmdRequest）包装成entry，交给raft层去同步。
其第二个参数是Callback类型，下层成功执行后用callback的done方法传入response来通知上层，上层便知道这个请求被执行并得到执行结果。并且如果处理过程中发生了错误，也需要将错误返回，错误通过ErrResp方法封装到RaftCmdResponse结构中。


proposals是用于记录Proposal的callback及任期等信息的结构，每个RaftCommand对应一个proposal，proposals在proposeRaftCommand中被append。每次处理RaftCommand之后会取出一个proposal，然后将返回的错误提交给它，之后会返回给客户端。
若指令正常执行，callback会在HandleRaftReady中使用，传递response。


#### HandleRaftReady
处理 rawNode 传递来的 Ready。
文档中给出了伪代码:
```
for {
  select {
  case <-s.Ticker:
    Node.Tick()
  default:
    if Node.HasReady() {
      rd := Node.Ready()
      saveToStorage(rd.State, rd.Entries, rd.Snapshot)
      send(rd.Messages)
      for _, entry := range rd.CommittedEntries {
        process(entry)
      }
      s.Node.Advance(rd)
    }
}
```
按照伪代码可以得到步骤如下：
1.调用hasReady，若没有Ready则返回。
2.调用 SaveReadyState 将 Ready 中需要持久化的内容保存到 badger。如果 Ready 中存在 snapshot，则应用它；
3.然后调用 d.Send() 方法将 Ready 中的 msg 发送出去；
4.应用 Ready 中的 CommittedEntries；
5.调用 d.RaftGroup.Advance() 推进 RawNode；

其中第四点应用entry过程较为复杂。下为逐一处理entry的步骤：
1.更新appliedIndex。
2.若entry_Type==normal entries，(entry_Type分为normal entries和configuration changes两种，此处只需要考虑normal entries)，使用 Unmarshal 方法将entry.Data解码成RaftCmdRequest结构。
3.遍历RaftCmdRequest中requests切片，根据cmd的类型（put，delete，get）进行分别处理，并分别设置response。
4.需要根据将RaftCmdRequest的处理结果通过proposal的CallBack返回。出现错误则返回错误。
5.写入需要通过engine_util.WriteBatch结果，以批量，原子地形式一次性将变化写入到状态机中（kvdb）


#### debug

##### Snap的回应
runtime error: invalid memory address or nil pointer dereference

虽然直到2b我们还没有实现snapshot的功能，但是测试集会测试req.CmdType == raft_cmdpb.CmdType_Snap时的response，由于snapshot的response含有一个region信息，若不对其赋值则会报错。
##### p.cb.Txn

在完成 Snap 条目的 apply 之后，需要给 p.cb.Txn 赋值一个可读事务。因为在 cluster.go 的 Scan() 方法中，会传给 Raft 一个 SnapCmd，然后通过回应的 txn 来构造迭代器。如果在执行完 SnapCmd 后没有给 p.cb.Txn 赋值，那么就会报 nil pointer 错误，代码如下：
``` go
p.cb.Txn = d.peerStorage.Engines.Kv.NewTransaction(false)
```
## 2c
若不做任何处理，随着我们当前设计的存储系统的长时间运行，Raft节点中的RaftLog会迅速增加，占用大量内存。因此我们需要实现SnapShot机制来对日志进行压缩和删除。
### SnapShot生成
此处不需要我们实现，但可以适当了解。
我们可以通过调用r.RaftLog.storage.Snapshot()来获取快照，而实际上该方法对于SnapShot的生成是异步的。将生成快照任务发送给ps.regionSched，并且将 snapState 的状态改为 Generating 表示正在产生快照。
这是由于快照比较大，可能耗时较长，调用storage.Snapshot()获取快照后，可能暂时无法获得快照，此时会返回err。等到下次再次获取快照时如果快照已经产生了，则会接收并返回快照。
### SnapShot发送与接收
SnapShot作为大量日志的替代品，在raft层的日志同步过程中会被使用到。

在发送entry过程中，可能发送快照：

1.当 Leader 发现要发给 Follower 的 entry 已经被压缩时（ Next < r.RaftLog.FirstIndex() ），就会通过 r.RaftLog.storage.Snapshot() 生成一份 Snapshot，并将生成的 Snapshot 发送给对应节点。
2.SnapShot异步生成，若超时则待下次发送时再次调用，若生成完毕，leader将SnapShot发送给follower。
3.每个节点有个成员叫 pendingSnapshot，为待应用的 Snapshot，如果 leader 发快照时pendingSnapshot 不为nil，直接发pendingSnapshot即可，不然通过第2步生成一个新的发过去。

follower也可能接收快照：

此处需要处理新的msg类型:MsgSnapshot。

**MsgSnapshot**

Common Msg，用于 Leader 将快照发送给其他节点，Project2C 中使用。成员如下：

| 成员     | 值                         |
| -------- | -------------------------- |
| MsgType  | pb.MessageType_MsgSnapshot |
| Term     | 当前节点的 Term            |
| Snapshot | 要发送的快照               |
| To       | 要发给谁                   |
| From     | 当前节点 ID                |

Follower 收到 Leader 发来的 pb.MessageType_MsgSnapshot 之后，会根据其中的 Metadata 来更新自己的 committed、applied、stabled 等等指针。然后将在 Snapshot 之前的 entry 均从 RaftLog.entries 中删除。之后，根据其中的 ConfState 更新自己的 Prs 信息。做完这些操作后，把 pendingSnapshot 置为该 Snapshot，等待 raftNode 通过 Ready( ) 交给 peer 层处理。

### SnapShot应用

在 HandleRaftReady( ) 中，如果收到的 Ready 包含 SnapState，就需要对其进行应用。调用链为HandleRadtReady() -> SaveReadyState() -> ApplySnaptshot()

ApplySnaptshot()实现：
 1.调用ps.clearMeta 和 ps.clearExtraData 来清空旧的数据。
 2.根据 Snapshot的term、index和region 更新 raftState 和 applyState。需要修改的分别为raftState的LastIndex 与 LastTerm 和applyState的AppliedIndex 以及 TruncatedState。
 3.持久化applyState、regionState和raftState。
 4.将snapState.StateType 赋值为 snap.SnapState_Applying
  5.. 生成一个 RegionTaskApply，传递给 ps.regionSched 管道。region_task.go 会接收该任务，然后异步的将 Snapshot 应用到 kvDB 中去。
### 日志压缩
1.在 HandleMsg( ) 会中收到 message.MsgTypeTick，然后进入 onTick( ) ，触发 d.onRaftGCLogTick( ) 方法。这个方法会检查 appliedIdx - firstIdx >= d.ctx.cfg.RaftLogGcCountLimit，如果是，就开始进行压缩。
2.调用proposeRaftCommand( )，将Raft 提交一个raftcmdRequest，包含 一个AdminRequest，类型为 AdminCmdType_CompactLog，将该 AdminRequest 封装成 entry 交给 raft 层来同步。
3.当该 entry 同步完成并需要被 apply 时，HandleRaftReady( ) 开始执行该条目中的命令 。这时会首先修改相关的状态，然后调用 d.ScheduleCompactLog( ) 发送 raftLogGCTask 任务给 raftlog_gc.go。raftlog_gc.go 收到后，会删除 raftDB 中对应 index 以及之前的所有已持久化的 entries，以实现压缩日志。



总的来说，当 entry 数量超过阈值时，就会触发 compactLog，compactLog 删除日志并更新 TruncatedState.Index，接着当 Leader 调用 sendAppend 时，若需要发送的日志被Truncated，才生成 snapshot。
### debug
#### 新增一个空条目
处理完快照之后要注意 lastIndex()实现，可能entries全部删除了，可能存在lastIndex() < snapshot.Metadata.Index，新增一个空条目，与snapshot的index一致。

#### can't get value xxxx for key xxxx 
```
panic: can't get value xxxx for key xxxx [recovered]
```

若出现这样的问题，可能是生成快照后，pendingsnapshot没有同步到engine，可能时没有设置pendingSnapshot，或Ready( )没有加入snapShot的处理。以及Advance()之后别忘了将pendingsnapshot设置为nil。。
